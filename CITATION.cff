cff-version: 1.2.0
title: >-
  Whispers in the Machine: Confidentiality in LLM-integrated
  Systems
message: >-
  If you want to cite our work or use this framework, please
  cite using the provided data.
type: software
authors:
  - given-names: Jonathan
    family-names: Evertz
    email: jonathan.evertz@cispa.de
    affiliation: CISPA Helmholtz Center for Information Security
  - given-names: Merlin
    family-names: Chlosta
    email: merlin.chlosta@cispa.de
    affiliation: CISPA Helmholtz Center for Information Security
  - given-names: Lea
    family-names: SchÃ¶nherr
    email: schoenherr@cispa.de
    affiliation: CISPA Helmholtz Center for Information Security
  - given-names: 'Thorsten '
    family-names: Eisenhofer
    email: thorsten.eisenhofer@tu-berlin.de
    affiliation: TU Berlin
identifiers:
  - type: url
    value: 'https://arxiv.org/abs/2402.06922'
repository-code: 'https://github.com/LostOxygen/llm-confidentiality'
abstract: >-
  Large Language Models (LLMs) are increasingly augmented with external tools and commercial services 
  into LLM-integrated systems. While these interfaces can significantly enhance the capabilities of the models, 
  they also introduce a new attack surface. Manipulated integrations, for example, can exploit the model and 
  compromise sensitive data accessed through other interfaces. While previous work primarily focused on attacks 
  targeting a model's alignment or the leakage of training data, the security of data that is only available during 
  inference has escaped scrutiny so far. In this work, we demonstrate the vulnerabilities associated with external 
  components and introduce a systematic approach to evaluate confidentiality risks in LLM-integrated systems. 
  
  We identify two specific attack scenarios unique to these systems and formalize these into a tool-robustness 
  framework designed to measure a model's ability to protect sensitive information. Our findings show that all 
  examined models are highly vulnerable to confidentiality attacks, with the risk increasing significantly when 
  models are used together with external tools.     
keywords:
  - large language models
  - llm
  - adversarial attacks
  - machine learning
  - confidentiality
  - prompt injections
  - llm security
license: Apache-2.0
